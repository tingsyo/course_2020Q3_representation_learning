{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for NOAA-GridSat-B1\n",
    "\n",
    "This notebook aims to develop and evaluate autoencoder for NOAA-GridSat-B1 data.\n",
    "\n",
    "\n",
    "## Autoencoder\n",
    "\n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. (from [wikipedia](https://en.wikipedia.org/wiki/Autoencoder))\n",
    "\n",
    "For advanced applications of AE, [this article](https://medium.com/%E7%A8%8B%E5%BC%8F%E5%B7%A5%E4%BD%9C%E7%B4%A1/autoencoder-%E4%B8%80-%E8%AA%8D%E8%AD%98%E8%88%87%E7%90%86%E8%A7%A3-725854ab25e8) introduced four variations of AE and their applications.\n",
    "\n",
    "\n",
    "### Factorization of the data dimension\n",
    "\n",
    "The dimension of the dataset used for analysis is (858,858), and we need to factorize the dimension in order to design a proper autoencoder.\n",
    "\n",
    "$858 = 2 \\times 3 \\times 11 \\times 13$\n",
    "\n",
    "\n",
    "### Utility functions for data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os, argparse, logging\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def list_noaagridsatb1_files(dir, suffix='.v02r01.nc', to_remove=['GRIDSAT-B1.','.v02r01.nc']):\n",
    "    ''' To scan through the sapecified dir and get the corresponding file with suffix. '''\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    xfiles = []\n",
    "    for root, dirs, files in os.walk(dir, followlinks=True):  # Loop through the directory\n",
    "        for fn in files:\n",
    "            if fn.endswith(suffix):         # Filter files with suffix\n",
    "                timestamp = fn\n",
    "                for s in to_remove:         # Removing prefix and suffix to get time-stamp\n",
    "                    timestamp = timestamp.replace(s,'')\n",
    "                xfiles.append({'timestamp':timestamp, 'xuri':os.path.join(root, fn)})\n",
    "    return(pd.DataFrame(xfiles).sort_values('timestamp').reset_index(drop=True))\n",
    "\n",
    "# Binary reader\n",
    "def read_noaagridsatb1(furi, var='irwin_cdr', scale=0.01, offset=200, remove_na=True, crop_east_asia=True):\n",
    "    ''' The method reads in a NOAA-GridSta-B1 image in netCDF4 format (.nc file). \n",
    "        The brightness temperature data was stored in int16 as 'irwin_cdr', with \n",
    "        a scal factor of 0.01 and offset of 200. The missing values is flagged as -31999.\n",
    "        More details of the data is described in https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_GRIDSAT-B1_V2.\n",
    "        Since our analysis focuss on East Asia (0-60'N, 100-160'E), we used an \n",
    "        option to crop the data to this region (index: lat:1000~1858, lon:4000~4858).\n",
    "        The output is a 2-d numpy array of float32 with shape (858, 858).\n",
    "    '''\n",
    "    import numpy as np\n",
    "    import netCDF4 as nc\n",
    "    # Read in data\n",
    "    data = nc.Dataset(furi)\n",
    "    cdr = np.array(data.variables['irwin_cdr'])*scale+offset\n",
    "    # Remove missing value\n",
    "    if remove_na:\n",
    "        cdr[cdr<0] = offset\n",
    "    # Crop domain to East-Asia (0-60'N, 100-160'E)\n",
    "    if crop_east_asia:\n",
    "        return(cdr[0, 1000:1858, 4000:4858])\n",
    "    else:\n",
    "        return(cdr[0,:,:])\n",
    "\n",
    "def read_multiple_noaagridsatb1(flist):\n",
    "    ''' This method reads in a list of NOAA-GridSat-B1 images and returns a numpy array. '''\n",
    "    import numpy as np\n",
    "    data = []\n",
    "    for f in flist:\n",
    "        tmp = read_noaagridsatb1(f)\n",
    "        if tmp is not None:\n",
    "            data.append(tmp)\n",
    "    return(np.array(data, dtype=np.float32))\n",
    "\n",
    "def data_generator_ae(flist, batch_size):\n",
    "    ''' Data generator for batched processing. '''\n",
    "    nSample = len(flist)\n",
    "    # This line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < nSample:\n",
    "            limit = min(batch_end, nSample)\n",
    "            X = read_multiple_noaagridsatb1(flist[batch_start:limit])\n",
    "            #print(X.shape)\n",
    "            yield (X,X) #a tuple with two numpy arrays with batch_size samples     \n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "    # End of generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few autoencoder models\n",
    "\n",
    "We design a few autoencoder models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder models\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def initialize_fc_autoencoder_noaagridsatb1(input_shape):\n",
    "    # Copressed dimension\n",
    "    latent_dim = 64\n",
    "    # Debug information\n",
    "    print('Input data dimension: '+str(input_shape))\n",
    "    print('Flatten data dimension: '+str(latent_dim))\n",
    "    # Define input layer\n",
    "    input_data = Input(shape=input_shape)\n",
    "    # Define encoder layers\n",
    "    x = Flatten()(input_data)\n",
    "    encoded = Dense(latent_dim, activation='relu')(x)\n",
    "    # Define decoder layers\n",
    "    x = Dense(np.prod(input_shape), activation='sigmoid')(encoded)\n",
    "    decoded = Reshape(input_shape)(x)\n",
    "    # Define autoencoder\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['cosine_similarity'])\n",
    "    # Encoder\n",
    "    encoder = Model(input_data, encoded)\n",
    "    return((autoencoder, encoder))\n",
    "\n",
    "\n",
    "def initialize_conv_autoencoder_noaagridsatb1(input_shape):\n",
    "    # Define input layer\n",
    "    input_data = Input(shape=input_shape)  # adapt this if using `channels_first` image data format\n",
    "    # Define encoder layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='encoder_conv1')(input_data)\n",
    "    x = MaxPooling2D((2, 2), name='encoder_maxpool1')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same', name='encoder_conv2')(x)\n",
    "    x = MaxPooling2D((3, 3), name='encoder_maxpool2')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same', name='encoder_conv3')(x)\n",
    "    encoded = MaxPooling2D((11, 11), name='encoder_maxpool3')(x)\n",
    "    # Define decoder layers\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same', name='decoder_conv1')(encoded)\n",
    "    x = UpSampling2D((11, 11), name='decoder_upsamp1')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same', name='decoder_conv2')(x)\n",
    "    x = UpSampling2D((3, 3), name='decoder_upsamp2')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='decoder_conv3')(x)\n",
    "    x = UpSampling2D((2, 2), name='decoder_upsamp4')(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', name='decoder_output', padding='same')(x)\n",
    "    # Define autoencoder\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['cosine_similarity'])\n",
    "    # Encoder\n",
    "    encoder = Model(input_data, encoded)\n",
    "    return((autoencoder, encoder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimension: (858, 858)\n",
      "Flatten data dimension: 64\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 858, 858)]        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 736164)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                47114560  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 736164)            47850660  \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 858, 858)          0         \n",
      "=================================================================\n",
      "Total params: 94,965,220\n",
      "Trainable params: 94,965,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training autoencoder with data size: 8\n",
      "Training data steps: 1.0\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 73336.0312 - cosine_similarity: 0.8038\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 73073.3203 - cosine_similarity: 0.9920\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 73066.1641 - cosine_similarity: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2817ba5b508>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters\n",
    "datadir = '../../data/noaa/'\n",
    "finfo = list_noaagridsatb1_files(datadir)\n",
    "\n",
    "NX = 858\n",
    "NY = 858\n",
    "batch_size = 32\n",
    "\n",
    "# Simple AE\n",
    "ae_fc = initialize_fc_autoencoder_noaagridsatb1((NY, NX))\n",
    "# Debug info\n",
    "nSample = finfo.shape[0]\n",
    "print(ae_fc[0].summary())\n",
    "print(\"Training autoencoder with data size: \"+str(nSample))\n",
    "steps_train = np.ceil(nSample/batch_size)\n",
    "print(\"Training data steps: \" + str(steps_train))\n",
    "# Fitting model\n",
    "ae_fc[0].fit_generator(data_generator_ae(finfo['xuri'], batch_size), steps_per_epoch=steps_train, epochs=3, max_queue_size=batch_size, use_multiprocessing=False, verbose=1)\n",
    "\n",
    "#ae_conv = initialize_conv_autoencoder_noaagridsatb1((NY, NX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

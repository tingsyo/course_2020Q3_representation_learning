{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for NOAA-GridSat-B1\n",
    "\n",
    "This notebook aims to develop and evaluate autoencoder for NOAA-GridSat-B1 data.\n",
    "\n",
    "\n",
    "## Autoencoder\n",
    "\n",
    "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. (from [wikipedia](https://en.wikipedia.org/wiki/Autoencoder))\n",
    "\n",
    "For advanced applications of AE, [this article](https://medium.com/%E7%A8%8B%E5%BC%8F%E5%B7%A5%E4%BD%9C%E7%B4%A1/autoencoder-%E4%B8%80-%E8%AA%8D%E8%AD%98%E8%88%87%E7%90%86%E8%A7%A3-725854ab25e8) introduced four variations of AE and their applications.\n",
    "\n",
    "\n",
    "### Factorization of the data dimension\n",
    "\n",
    "The dimension of the dataset used for analysis is (858,858), and we need to factorize the dimension in order to design a proper autoencoder.\n",
    "\n",
    "$858 = 2 \\times 3 \\times 11 \\times 13$\n",
    "\n",
    "\n",
    "### Utility functions for data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os, argparse, logging\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def list_noaagridsatb1_files(dir, suffix='.v02r01.nc', to_remove=['GRIDSAT-B1.','.v02r01.nc']):\n",
    "    ''' To scan through the sapecified dir and get the corresponding file with suffix. '''\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    xfiles = []\n",
    "    for root, dirs, files in os.walk(dir, followlinks=True):  # Loop through the directory\n",
    "        for fn in files:\n",
    "            if fn.endswith(suffix):         # Filter files with suffix\n",
    "                timestamp = fn\n",
    "                for s in to_remove:         # Removing prefix and suffix to get time-stamp\n",
    "                    timestamp = timestamp.replace(s,'')\n",
    "                xfiles.append({'timestamp':timestamp, 'xuri':os.path.join(root, fn)})\n",
    "    return(pd.DataFrame(xfiles).sort_values('timestamp').reset_index(drop=True))\n",
    "\n",
    "# Binary reader\n",
    "def read_noaagridsatb1(furi, var='irwin_cdr', scale=0.01, offset=200, remove_na=True, crop_east_asia=True):\n",
    "    ''' The method reads in a NOAA-GridSta-B1 image in netCDF4 format (.nc file). \n",
    "        The brightness temperature data was stored in int16 as 'irwin_cdr', with \n",
    "        a scal factor of 0.01 and offset of 200. The missing values is flagged as -31999.\n",
    "        More details of the data is described in https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_GRIDSAT-B1_V2.\n",
    "        Since our analysis focuss on East Asia (0-60'N, 100-160'E), we used an \n",
    "        option to crop the data to this region (index: lat:1000~1858, lon:4000~4858).\n",
    "        The output is a 2-d numpy array of float32 with shape (858, 858).\n",
    "    '''\n",
    "    import numpy as np\n",
    "    import netCDF4 as nc\n",
    "    # Read in data\n",
    "    data = nc.Dataset(furi)\n",
    "    cdr = np.array(data.variables['irwin_cdr'])*scale+offset\n",
    "    # Remove missing value\n",
    "    if remove_na:\n",
    "        cdr[cdr<0] = offset\n",
    "    # Crop domain to East-Asia (0-60'N, 100-160'E)\n",
    "    if crop_east_asia:\n",
    "        return(cdr[0, 1000:1858, 4000:4858])\n",
    "    else:\n",
    "        return(cdr[0,:,:])\n",
    "\n",
    "def read_multiple_noaagridsatb1(flist):\n",
    "    ''' This method reads in a list of NOAA-GridSat-B1 images and returns a numpy array. '''\n",
    "    import numpy as np\n",
    "    data = []\n",
    "    for f in flist:\n",
    "        tmp = read_noaagridsatb1(f)\n",
    "        if tmp is not None:\n",
    "            data.append(tmp)\n",
    "    return(np.array(data, dtype=np.float32))\n",
    "\n",
    "def data_generator_ae(flist, batch_size):\n",
    "    ''' Data generator for batched processing. '''\n",
    "    nSample = len(flist)\n",
    "    # This line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < nSample:\n",
    "            limit = min(batch_end, nSample)\n",
    "            X = read_multiple_noaagridsatb1(flist[batch_start:limit])\n",
    "            #print(X.shape)\n",
    "            yield (X,X) #a tuple with two numpy arrays with batch_size samples     \n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "    # End of generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few autoencoder models\n",
    "\n",
    "We design a few autoencoder models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder models\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def initialize_fc_autoencoder_noaagridsatb1(input_shape):\n",
    "    \n",
    "\n",
    "\n",
    "def initialize_conv_autoencoder_noaagridsatb1(input_shape):\n",
    "    # Define input layer\n",
    "    input_data = Input(shape=input_shape)  # adapt this if using `channels_first` image data format\n",
    "    # Define encoder layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='encoder_conv1', data_format='channels_first')(input_data)\n",
    "    x = MaxPooling2D((5, 3), name='encoder_maxpool1', data_format='channels_first')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same', name='encoder_conv2', data_format='channels_first')(x)\n",
    "    x = MaxPooling2D((1, 3), name='encoder_maxpool2', data_format='channels_first')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same', name='encoder_conv3', data_format='channels_first')(x)\n",
    "    x = MaxPooling2D((1, 3), name='encoder_maxpool3', data_format='channels_first')(x)\n",
    "    x = Conv2D(4, (3, 3), activation='relu', padding='same', name='encoder_conv4', data_format='channels_first')(x)\n",
    "    encoded = MaxPooling2D((5, 3), name='encoder_maxpool4', data_format='channels_first')(x)\n",
    "    #encoded = x\n",
    "    # Define decoder layers\n",
    "    x = Conv2D(4, (3, 3), activation='relu', padding='same', name='decoder_conv1', data_format='channels_first')(encoded)\n",
    "    x = UpSampling2D((5, 3), name='decoder_upsamp1', data_format='channels_first')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same', name='decoder_conv2', data_format='channels_first')(x)\n",
    "    x = UpSampling2D((1, 3), name='decoder_upsamp2', data_format='channels_first')(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same', name='decoder_conv3', data_format='channels_first')(x)\n",
    "    x = UpSampling2D((1, 3), name='decoder_upsamp3', data_format='channels_first')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', name='decoder_conv4', data_format='channels_first')(x)\n",
    "    x = UpSampling2D((5, 3), name='decoder_upsamp4', data_format='channels_first')(x)\n",
    "    decoded = Conv2D(6, (3, 3), activation='sigmoid', name='decoder_output', padding='same', data_format='channels_first')(x)\n",
    "    # Define autoencoder\n",
    "    autoencoder = Model(input_data, decoded)\n",
    "    #autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['cosine_similarity'])\n",
    "    # Encoder\n",
    "    encoder = Model(input_data, encoded)\n",
    "    return((autoencoder, encoder))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
